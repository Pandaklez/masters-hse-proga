{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "name": "MT_transformer_torch_Klezovich.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13efba76"
      },
      "source": [
        "# Дисклеймер\n",
        "Эту тетрадку нужно запускать в колабе или в vast.ai. Не мучатесь с установкой библиотек и с обучением на cpu."
      ],
      "id": "13efba76"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d650e9eb",
        "scrolled": true,
        "outputId": "ad6e7e9d-381e-4cd0-85c3-dd37f415fa15"
      },
      "source": [
        "!pip install tokenizers matplotlib sklearn"
      ],
      "id": "d650e9eb",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tokenizers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/e2/df3543e8ffdab68f5acc73f613de9c2b155ac47f162e725dcac87c521c11/tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 6.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (3.2.2)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (0.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (0.10.0)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.19.5)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.8.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.4.7)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn) (0.22.2.post1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib) (1.15.0)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.0.1)\n",
            "Installing collected packages: tokenizers\n",
            "Successfully installed tokenizers-0.10.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ac85a206"
      },
      "source": [
        "# в vast ai или в последних версия jupyter может не работать автозаполнение, установка этой либы и перезагрука кернела помогает\n",
        "# !pip install --upgrade jedi==0.17.2"
      ],
      "id": "ac85a206",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e384bd5c"
      },
      "source": [
        "# Транформеры для решения seq2seq задач"
      ],
      "id": "e384bd5c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a69baacc"
      },
      "source": [
        "Seq2seq - наверное самая общая формальная постановка задачи в NLP. Нужно из произвольной последовательности получить какую-то другую последовательность. И в отличие от разметки последовательности (sequence labelling) не требуется, чтобы обе последовательности совпадали по длине. Даже стандартную задачу классификации можно решать как seq2seq - можно рассматривать метку класса как последовательность длинны 1."
      ],
      "id": "a69baacc"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fe2a0525"
      },
      "source": [
        "А трансформеры - sota архитектура для seq2seq задач. Мы не будем подробно разбирать устройство транформеров, если вам интересно вы можете поразбираться вот с этими материалами:\n",
        "\n",
        "Оригинальная статья (сложновато) - https://arxiv.org/pdf/1706.03762.pdf\n",
        "\n",
        "https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/  \n",
        "https://jalammar.github.io/illustrated-transformer/\n",
        "\n",
        "https://www.youtube.com/watch?v=iDulhoQ2pro\n",
        "\n",
        "https://www.youtube.com/watch?v=TQQlZhbC5ps\n",
        "\n",
        "Самый известный туториал (на торче) - https://nlp.seas.harvard.edu/2018/04/03/attention.html\n",
        "\n",
        "\n",
        "\n",
        "Трансформеры будут подробно разбираться на курсе глубокого обучения (по выбору) на втором курсе."
      ],
      "id": "fe2a0525"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46c794a6"
      },
      "source": [
        "Пока просто попробуем обучать модель на задаче машинного перевода. Для таких задач лучше всего использовать предобученные модели, но если у вас будет какая-то специфичная seq2seq задача, то имеет смысл попробовать обучить трансформер с нуля и в этой тертрадке вам нужно будет поменять только часть с загрузкой данных. "
      ],
      "id": "46c794a6"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "DejrQ6fPBnY4",
        "outputId": "3dc52447-e554-4fa7-df47-94fdc537eabf"
      },
      "source": [
        "!pip install torch==1.8.1"
      ],
      "id": "DejrQ6fPBnY4",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torch==1.8.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/56/74/6fc9dee50f7c93d6b7d9644554bdc9692f3023fa5d1de779666e6bf8ae76/torch-1.8.1-cp37-cp37m-manylinux1_x86_64.whl (804.1MB)\n",
            "\u001b[K     |████████████████████████████████| 804.1MB 22kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.8.1) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.1) (3.7.4.3)\n",
            "\u001b[31mERROR: torchvision 0.10.0+cu102 has requirement torch==1.9.0, but you'll have torch 1.8.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: torchtext 0.10.0 has requirement torch==1.9.0, but you'll have torch 1.8.1 which is incompatible.\u001b[0m\n",
            "Installing collected packages: torch\n",
            "  Found existing installation: torch 1.9.0+cu102\n",
            "    Uninstalling torch-1.9.0+cu102:\n",
            "      Successfully uninstalled torch-1.9.0+cu102\n",
            "Successfully installed torch-1.8.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "torch"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "947b3313"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "\n",
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import StratifiedShuffleSplit, train_test_split\n",
        "from string import punctuation\n",
        "from collections import Counter\n",
        "from IPython.display import Image\n",
        "from IPython.core.display import HTML \n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "id": "947b3313",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "415f5ed2"
      },
      "source": [
        "# в русскоязычных данных есть \\xa0 вместо пробелов, он может некорректно обрабатываться токенизатором\n",
        "# text = open('opus.en-ru-train.ru.txt').read().replace('\\xa0', ' ')\n",
        "# text = open('opus.en-ru-train.ru.txt').read().replace('\\xa0', ' ')\n",
        "# f = open('opus.en-ru-train.ru.txt', 'w')\n",
        "# f.write(text)\n",
        "# f.close()"
      ],
      "id": "415f5ed2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "018d83aa"
      },
      "source": [
        "Данные взяты вот отсюда - https://opus.nlpl.eu/opus-100.php (раздел с отдельными языковыми парами)"
      ],
      "id": "018d83aa"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e110ff04"
      },
      "source": [
        "en_sents = open('opus.en-ru-train.en.txt').read().splitlines()\n",
        "ru_sents = open('opus.en-ru-train.ru.txt').read().replace('\\xa0', ' ').splitlines()"
      ],
      "id": "e110ff04",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c009c96e"
      },
      "source": [
        "Пример перевода с английского на русский"
      ],
      "id": "c009c96e"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0eb9b498",
        "outputId": "6b7bb2e3-a47b-4603-8a79-1ff02b095148"
      },
      "source": [
        "en_sents[-5], ru_sents[-5]"
      ],
      "id": "0eb9b498",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('My humblest apologies.', 'Нижайше прошу прощения.')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c39921c4"
      },
      "source": [
        "Как обычно нам нужен токенизатор, а точнее даже 2, т.к. у нас два корпуса"
      ],
      "id": "c39921c4"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4b79b4da"
      },
      "source": [
        "#tokenizer_en = Tokenizer(BPE())\n",
        "#tokenizer_en.pre_tokenizer = Whitespace()\n",
        "#trainer_en = BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
        "#tokenizer_en.train(files=[\"opus.en-ru-train.en.txt\"], trainer=trainer_en)\n",
        "\n",
        "#tokenizer_ru = Tokenizer(BPE())\n",
        "#tokenizer_ru.pre_tokenizer = Whitespace()\n",
        "#trainer_ru = BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
        "#tokenizer_ru.train(files=[\"opus.en-ru-train.ru.txt\"], trainer=trainer_ru)"
      ],
      "id": "4b79b4da",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48780b24"
      },
      "source": [
        "### ВАЖНО!"
      ],
      "id": "48780b24"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1b56d3b"
      },
      "source": [
        "Токенизатор - это неотъемлимая часть модели, поэтому не забывайте сохранять токенизатор вместе с моделью. Если вы забудете про это и переобучите токенизатор, то индексы токенов разойдутся и веса модели будут бесполезны. "
      ],
      "id": "f1b56d3b"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0dd90665"
      },
      "source": [
        "# раскоментируйте эту ячейку при обучении токенизатора\n",
        "# а потом снова закоментируйте чтобы при перезапуске не перезаписать токенизаторы\n",
        "#tokenizer_en.save('tokenizer_en')\n",
        "#tokenizer_ru.save('tokenizer_ru')"
      ],
      "id": "0dd90665",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0e0f7f77"
      },
      "source": [
        "tokenizer_en = Tokenizer.from_file(\"tokenizer_en\")\n",
        "tokenizer_ru = Tokenizer.from_file(\"tokenizer_ru\")"
      ],
      "id": "0e0f7f77",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "af9fecf3"
      },
      "source": [
        "Переводим текст в индексы вот таким образом. В начало добавляем токен '[CLS]', а в конец '[SEP]'. Если вспомните занятие по языковому моделированию, то там мы добавляли \"\\<start>\" и \"\\<end>\" - cls и sep по сути тоже самое. Вы поймете почему именно cls и sep, а не start и end, если подробнее поразбираетесь с устройством трансформеров"
      ],
      "id": "af9fecf3"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dc003758"
      },
      "source": [
        "def encode(text, tokenizer, max_len):\n",
        "    return [tokenizer.token_to_id('[CLS]')] + tokenizer.encode(text).ids[:max_len] + [tokenizer.token_to_id('[SEP]')]"
      ],
      "id": "dc003758",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96920fdc",
        "outputId": "f62cd62e-5725-43fc-81ce-824632a81209"
      },
      "source": [
        "# важно следить чтобы индекс паддинга совпадал в токенизаторе с value в pad_sequences\n",
        "PAD_IDX = tokenizer_ru.token_to_id('[PAD]')\n",
        "PAD_IDX"
      ],
      "id": "96920fdc",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5cc0a376"
      },
      "source": [
        "# ограничимся длинной в 30 и 35 (разные чтобы показать что в seq2seq не нужна одинаковая длина)\n",
        "max_len_en, max_len_ru = 30, 35"
      ],
      "id": "5cc0a376",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fc2dae1"
      },
      "source": [
        "X_en = [encode(t, tokenizer_en, max_len_en) for t in en_sents]\n",
        "X_ru = [encode(t, tokenizer_ru, max_len_ru) for t in ru_sents]"
      ],
      "id": "7fc2dae1",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0560e01f",
        "outputId": "61eab926-3c94-4fbd-d098-cc2c8c5afcdc"
      },
      "source": [
        "# миллион примеров \n",
        "len(X_en), len(X_ru)"
      ],
      "id": "0560e01f",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1000000, 1000000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68091524"
      },
      "source": [
        ""
      ],
      "id": "68091524",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7655a4ea"
      },
      "source": [
        "Паддинг внутри класса для датасета. Еще обратите внимание, что тут не стоит параметр batch_first=True как раньше\n",
        "\n",
        "В торче принято, что размерность батча идет в конце и пример кода с трансформером расчитан на это. Конечно можно поменять сам код модели, но это сложнее, чем просто изменить тензор с данными."
      ],
      "id": "7655a4ea"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7634853b"
      },
      "source": [
        "class Dataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, texts_en, texts_ru):\n",
        "        self.texts_en = [torch.LongTensor(sent) for sent in texts_en]\n",
        "        self.texts_en = torch.nn.utils.rnn.pad_sequence(self.texts_en, padding_value=PAD_IDX)\n",
        "        \n",
        "        self.texts_ru = [torch.LongTensor(sent) for sent in texts_ru]\n",
        "        self.texts_ru = torch.nn.utils.rnn.pad_sequence(self.texts_ru, padding_value=PAD_IDX)\n",
        "\n",
        "        self.length = len(texts_en)\n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        ids_en = self.texts_en[:, index]\n",
        "        ids_ru = self.texts_ru[:, index]\n",
        "\n",
        "        return ids_en, ids_ru"
      ],
      "id": "7634853b",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ec889a8d"
      },
      "source": [
        "Разбиваем на трейн и тест"
      ],
      "id": "ec889a8d"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9c9eaf09"
      },
      "source": [
        "X_en_train, X_en_valid, X_ru_train, X_ru_valid = train_test_split(X_en, X_ru, test_size=0.05)"
      ],
      "id": "9c9eaf09",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2f0fcb7a"
      },
      "source": [
        "training_set = Dataset(X_en_train, X_ru_train)\n",
        "training_generator = torch.utils.data.DataLoader(training_set, batch_size=200, shuffle=True)"
      ],
      "id": "2f0fcb7a",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6d0980a4"
      },
      "source": [
        "valid_set = Dataset(X_en_valid, X_ru_valid)\n",
        "valid_generator = torch.utils.data.DataLoader(valid_set, batch_size=200, shuffle=True)"
      ],
      "id": "6d0980a4",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32bb0e70"
      },
      "source": [
        "# Код трансформера"
      ],
      "id": "32bb0e70"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54a83a16"
      },
      "source": [
        "Дальше код модели, он взят вот отсюда (с небольшими изменениями) - https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
        "\n",
        "Там есть комментарии по каждому этапу"
      ],
      "id": "54a83a16"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "078605bf"
      },
      "source": [
        "from torch import Tensor\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import Transformer\n",
        "import math\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# helper Module that adds positional encoding to the token embedding to introduce a notion of word order.\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self,\n",
        "                 emb_size: int,\n",
        "                 dropout: float,\n",
        "                 maxlen: int = 150):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n",
        "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
        "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
        "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
        "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
        "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer('pos_embedding', pos_embedding)\n",
        "\n",
        "    def forward(self, token_embedding: Tensor):\n",
        "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n",
        "\n",
        "# helper Module to convert tensor of input indices into corresponding tensor of token embeddings\n",
        "class TokenEmbedding(nn.Module):\n",
        "    def __init__(self, vocab_size: int, emb_size):\n",
        "        super(TokenEmbedding, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
        "        self.emb_size = emb_size\n",
        "\n",
        "    def forward(self, tokens: Tensor):\n",
        "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n",
        "\n",
        "# Seq2Seq Network\n",
        "class Seq2SeqTransformer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 num_encoder_layers: int,\n",
        "                 num_decoder_layers: int,\n",
        "                 emb_size: int,\n",
        "                 nhead: int,\n",
        "                 src_vocab_size: int,\n",
        "                 tgt_vocab_size: int,\n",
        "                 dim_feedforward: int = 512,\n",
        "                 dropout: float = 0.1):\n",
        "        super(Seq2SeqTransformer, self).__init__()\n",
        "        self.transformer = Transformer(d_model=emb_size, \n",
        "                                       nhead=nhead,\n",
        "                                       num_encoder_layers=num_encoder_layers,\n",
        "                                       num_decoder_layers=num_decoder_layers,\n",
        "                                       dim_feedforward=dim_feedforward,\n",
        "                                       dropout=dropout)\n",
        "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
        "        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n",
        "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n",
        "        self.positional_encoding = PositionalEncoding(\n",
        "            emb_size, dropout=dropout)\n",
        "\n",
        "    def forward(self,\n",
        "                src: Tensor,\n",
        "                trg: Tensor,\n",
        "                src_mask: Tensor,\n",
        "                tgt_mask: Tensor,\n",
        "                src_padding_mask: Tensor,\n",
        "                tgt_padding_mask: Tensor,\n",
        "                memory_key_padding_mask: Tensor):\n",
        "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
        "#         print('pos inp')\n",
        "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n",
        "#         print('pos dec')\n",
        "        outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None,\n",
        "                                src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\n",
        "#         print('pos out')\n",
        "        x = self.generator(outs)\n",
        "#         print('gen')\n",
        "        return x\n",
        "\n",
        "    def encode(self, src: Tensor, src_mask: Tensor):\n",
        "        return self.transformer.encoder(self.positional_encoding(\n",
        "                            self.src_tok_emb(src)), src_mask)\n",
        "\n",
        "    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n",
        "        return self.transformer.decoder(self.positional_encoding(\n",
        "                          self.tgt_tok_emb(tgt)), memory,\n",
        "                          tgt_mask)\n",
        "# During training, we need a subsequent word mask that will prevent model to look into the future words when making predictions. We will also need masks to hide source and target padding tokens. Below, let’s define a function that will take care of both.\n",
        "\n",
        "def generate_square_subsequent_mask(sz):\n",
        "    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n",
        "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "    return mask\n",
        "\n",
        "\n",
        "def create_mask(src, tgt):\n",
        "    src_seq_len = src.shape[0]\n",
        "    tgt_seq_len = tgt.shape[0]\n",
        "\n",
        "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
        "    src_mask = torch.zeros((src_seq_len, src_seq_len),device=DEVICE).type(torch.bool)\n",
        "\n",
        "    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
        "    tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)\n",
        "    \n",
        "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"
      ],
      "id": "078605bf",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ae05d7f6"
      },
      "source": [
        "Обратите внимание на то как мы подаем данные в модель"
      ],
      "id": "ae05d7f6"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dedf9014"
      },
      "source": [
        "from time import time\n",
        "def train(model, iterator, optimizer, criterion, print_every=10):\n",
        "    \n",
        "    epoch_loss = []\n",
        "    ac = []\n",
        "    \n",
        "    model.train()  \n",
        "\n",
        "    for i, (texts_en, texts_ru) in enumerate(iterator):\n",
        "        texts_en = texts_en.T.to(DEVICE) # чтобы батч был в конце\n",
        "        texts_ru = texts_ru.T.to(DEVICE) # чтобы батч был в конце\n",
        "        \n",
        "        # помимо текста в модель еще нужно передать целевую последовательность\n",
        "        # но не полную а без 1 последнего элемента\n",
        "        # а на выходе ожидаем, что модель сгенерирует этот недостающий элемент\n",
        "        texts_ru_input = texts_ru[:-1, :]\n",
        "        \n",
        "        \n",
        "        # в трансформерах нет циклов как в лстм \n",
        "        # каждый элемент связан с каждым через аттеншен\n",
        "        # чтобы имитировать последовательную обработку\n",
        "        # и чтобы не считать аттеншн с паддингом \n",
        "        # в трансформерах нужно считать много масок\n",
        "        # подробнее про это по ссылкам выше\n",
        "        (texts_en_mask, texts_ru_mask, \n",
        "        texts_en_padding_mask, texts_ru_padding_mask) = create_mask(texts_en, texts_ru_input)\n",
        "        logits = model(texts_en, texts_ru_input, texts_en_mask, texts_ru_mask,\n",
        "                       texts_en_padding_mask, texts_ru_padding_mask, texts_en_padding_mask)\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # сравниваем выход из модели с целевой последовательностью уже с этим последним элементом\n",
        "        texts_ru_out = texts_ru[1:, :]\n",
        "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), texts_ru_out.reshape(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss.append(loss.item())\n",
        "        \n",
        "        if not (i+1) % print_every:\n",
        "            print(f'Loss: {np.mean(epoch_loss)};')\n",
        "        \n",
        "    return np.mean(epoch_loss)\n",
        "\n",
        "\n",
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    epoch_loss = []\n",
        "    epoch_f1 = []\n",
        "    \n",
        "    model.eval()  \n",
        "    with torch.no_grad():\n",
        "        for i, (texts_en, texts_ru) in enumerate(iterator):\n",
        "            texts_en = texts_en.T.to(DEVICE)\n",
        "            texts_ru = texts_ru.T.to(DEVICE)\n",
        "\n",
        "            texts_ru_input = texts_ru[:-1, :]\n",
        "\n",
        "            (texts_en_mask, texts_ru_mask, \n",
        "            texts_en_padding_mask, texts_ru_padding_mask) = create_mask(texts_en, texts_ru_input)\n",
        "\n",
        "            logits = model(texts_en, texts_ru_input, texts_en_mask, texts_ru_mask,\n",
        "                           texts_en_padding_mask, texts_ru_padding_mask, texts_en_padding_mask)\n",
        "\n",
        "            \n",
        "            texts_ru_out = texts_ru[1:, :]\n",
        "            loss = loss_fn(logits.reshape(-1, logits.shape[-1]), texts_ru_out.reshape(-1))\n",
        "            epoch_loss.append(loss.item())\n",
        "            \n",
        "    return np.mean(epoch_loss)"
      ],
      "id": "dedf9014",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b697183c"
      },
      "source": [
        ""
      ],
      "id": "b697183c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4ea391c"
      },
      "source": [
        "torch.manual_seed(0)\n",
        "\n",
        "EN_VOCAB_SIZE = tokenizer_en.get_vocab_size()\n",
        "RU_VOCAB_SIZE = tokenizer_ru.get_vocab_size()\n",
        "\n",
        "EMB_SIZE = 256\n",
        "NHEAD = 8\n",
        "FFN_HID_DIM = 512\n",
        "NUM_ENCODER_LAYERS = 2\n",
        "NUM_DECODER_LAYERS = 2\n",
        "\n",
        "transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n",
        "                                 NHEAD, EN_VOCAB_SIZE, RU_VOCAB_SIZE, FFN_HID_DIM)\n",
        "\n",
        "for p in transformer.parameters():\n",
        "    if p.dim() > 1:\n",
        "        nn.init.xavier_uniform_(p)\n",
        "\n",
        "transformer = transformer.to(DEVICE)\n",
        "\n",
        "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX).to(DEVICE)\n",
        "\n",
        "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"
      ],
      "id": "c4ea391c",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07f5b3a7"
      },
      "source": [
        "#torch.save(transformer, 'model')"
      ],
      "id": "07f5b3a7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d39a4011"
      },
      "source": [
        "# веса, которые скинули в чат. У меня у самой учится одна эпоха 4 часа\n",
        "transformer = torch.load('model', map_location=DEVICE)"
      ],
      "id": "d39a4011",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5559362d"
      },
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "id": "5559362d",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "id": "2070ab9c",
        "outputId": "f570b8f2-0f12-42ed-aae4-336aa1f6b9fc"
      },
      "source": [
        "from timeit import default_timer as timer\n",
        "NUM_EPOCHS = 4\n",
        "\n",
        "losses = []\n",
        "\n",
        "for epoch in range(1, NUM_EPOCHS+1):\n",
        "    start_time = timer()\n",
        "    train_loss = train(transformer, training_generator, optimizer, loss_fn)\n",
        "    end_time = timer()\n",
        "    val_loss = evaluate(transformer, valid_generator, loss_fn)\n",
        "    \n",
        "    if not losses:\n",
        "        print(f'First epoch - {val_loss}, saving model..')\n",
        "        torch.save(transformer, 'model')\n",
        "    \n",
        "    elif val_loss < min(losses):\n",
        "        print(f'Improved from {min(losses)} to {val_loss}, saving model..')\n",
        "        torch.save(transformer, 'model')\n",
        "    \n",
        "    losses.append(val_loss)\n",
        "        \n",
        "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \\\n",
        "           \"f\"Epoch time={(end_time-start_time):.3f}s\"))"
      ],
      "id": "2070ab9c",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-5e173976df45>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-75807e85bb82>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, iterator, optimizer, criterion, print_every)\u001b[0m\n\u001b[1;32m     26\u001b[0m         texts_en_padding_mask, texts_ru_padding_mask) = create_mask(texts_en, texts_ru_input)\n\u001b[1;32m     27\u001b[0m         logits = model(texts_en, texts_ru_input, texts_en_mask, texts_ru_mask,\n\u001b[0;32m---> 28\u001b[0;31m                        texts_en_padding_mask, texts_ru_padding_mask, texts_en_padding_mask)\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-216c3930c48d>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, trg, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\u001b[0m\n\u001b[1;32m     75\u001b[0m                                 src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\n\u001b[1;32m     76\u001b[0m \u001b[0;31m#         print('pos out')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;31m#         print('gen')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1751\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1753\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1754\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1755\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8NJr6qgDasJ"
      },
      "source": [
        ""
      ],
      "id": "g8NJr6qgDasJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFC03Um4Dav2"
      },
      "source": [
        "with open('bellingcat.txt', encoding='utf') as f:\n",
        "  en_text = f.read()"
      ],
      "id": "HFC03Um4Dav2",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "sW_j1q9hJRMr",
        "outputId": "607068f6-657b-43ce-ff23-b888304edad5"
      },
      "source": [
        "en_text[:491]"
      ],
      "id": "sW_j1q9hJRMr",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Russian Poet Dmitry Bykov Targeted by Navalny Poisoners\\nJune 9, 2021\\nFSB\\nRussia\\nIn a December 2020 investigation, Bellingcat and its partners identified seven FSB officers who had tailed Russian opposition leader Alexey Navalny on more than 35 trips around Russia since early 2017. Three members of this group – which included chemical weapons experts, medical doctors and security operatives – had shadowed Navalny to Novosibirsk and onward to Tomsk, during his August 2020 trip to Siberia.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zhGDzKSkNt6s",
        "outputId": "6a310683-55b0-4a39-b18b-90314743a0cd"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "id": "zhGDzKSkNt6s",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_CZUiqNEN5gG",
        "outputId": "059a3388-8fd9-4ecc-d747-a79eab8c9967"
      },
      "source": [
        "a_list = nltk.tokenize.sent_tokenize(en_text)\n",
        "a_list[:10]"
      ],
      "id": "_CZUiqNEN5gG",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Russian Poet Dmitry Bykov Targeted by Navalny Poisoners\\nJune 9, 2021\\nFSB\\nRussia\\nIn a December 2020 investigation, Bellingcat and its partners identified seven FSB officers who had tailed Russian opposition leader Alexey Navalny on more than 35 trips around Russia since early 2017.',\n",
              " 'Three members of this group – which included chemical weapons experts, medical doctors and security operatives – had shadowed Navalny to Novosibirsk and onward to Tomsk, during his August 2020 trip to Siberia.',\n",
              " 'Navalny fell into a near-fatal coma on a flight from Tomsk to Moscow on 21 August 2020, the result of what three European laboratories and the OPCW later identified as severe Novichok poisoning.',\n",
              " 'The telephone of one member of the FSB team was geolocated within walking distance of the hotel Navalny was staying the night before he fell ill.\\nDuring a phone call with Navalny, one of the members of the FSB team – Konstantin Kudryavtsev, a chemical engineer with a military chemical weapons background, stated that the FSB team poisoned the Russian politician, and subsequently tried to cover its tracks by removing traces of the nerve agent from the victim’s clothes.',\n",
              " 'Subsequent investigations by Bellingcat identified significant correlations between the travels of members of this FSB squad, and previously unexplained poisonings or deaths of several other public figures – including the twice near-fatal poisoning of outspoken opposition politician Vladimir Kara-Murza.',\n",
              " 'Other likely targets included two human rights activists in the Caucasus as well as an anti-corruption activist.',\n",
              " 'As the number of investigated cases grew, a pattern emerged in each poisoning case showing collaborative action of two FSB departments: secret service operatives from the anti-extremism department of the FSB’s Second Service, on one hand, and chemical weapons specialists with chemistry or medical backgrounds from FSB’s Criminalistics Institute, on the other.',\n",
              " 'In the early stages, a poisoning target would be tailed by members of the Second Service, while the Criminalistics Institute experts would typically join them in the latter, implementation phase of each poisoning operation.',\n",
              " 'In each case, members from both FSB units participated in the final, operational trips during which the victim was poisoned.',\n",
              " 'In addition, in each of the cases identified by Bellingcat, the poisoning took place outside of Moscow, typically during the victim’s trip to a provincial town.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PiEINSxFRFVx"
      },
      "source": [
        "Короче вот работающий код, но Михаил отвечал нам, что extend неправильно использовать. но я не стану совсем удалять, потому что дальше у меня вообще не получилось((("
      ],
      "id": "PiEINSxFRFVx"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4a21a9c8"
      },
      "source": [
        "def translate_works(full_text):\n",
        "\n",
        "    sent_text = nltk.tokenize.sent_tokenize(full_text)\n",
        "    outputs = [] \n",
        "    inputs = []  # тут один тензор\n",
        "\n",
        "    k = 0\n",
        "    for text in sent_text:\n",
        "      input_ids = [tokenizer_en.token_to_id('[CLS]')] + tokenizer_en.encode(text).ids[:max_len_en] + [tokenizer_en.token_to_id('[SEP]')]\n",
        "      output_ids = [tokenizer_ru.token_to_id('[CLS]')]\n",
        "      inputs.extend(input_ids)\n",
        "      outputs.extend(output_ids)\n",
        "      k += 1\n",
        "\n",
        "    input_ids_pad = torch.nn.utils.rnn.pad_sequence([torch.LongTensor(inputs)]).to(DEVICE)\n",
        "    output_ids_pad = torch.nn.utils.rnn.pad_sequence([torch.LongTensor(outputs)]).to(DEVICE)\n",
        "\n",
        "    (texts_en_mask, texts_ru_mask, \n",
        "    texts_en_padding_mask, texts_ru_padding_mask) = create_mask(input_ids_pad, output_ids_pad)\n",
        "    logits = transformer(input_ids_pad, output_ids_pad, texts_en_mask, texts_ru_mask,\n",
        "                         texts_en_padding_mask, texts_ru_padding_mask, texts_en_padding_mask)\n",
        "    \n",
        "    for logit in logits.argmax(2):\n",
        "      pred = logit.item()\n",
        "\n",
        "      while pred not in [tokenizer_ru.token_to_id('[SEP]'), tokenizer_ru.token_to_id('[PAD]')]:\n",
        "        outputs.append(pred)\n",
        "        output_ids_pad = torch.nn.utils.rnn.pad_sequence([torch.LongTensor(outputs)]).to(DEVICE)\n",
        "\n",
        "        (texts_en_mask, texts_ru_mask, \n",
        "        texts_en_padding_mask, texts_ru_padding_mask) = create_mask(input_ids_pad, output_ids_pad)\n",
        "        logits = transformer(input_ids_pad, output_ids_pad, texts_en_mask, texts_ru_mask,\n",
        "                             texts_en_padding_mask, texts_ru_padding_mask, texts_en_padding_mask)\n",
        "        pred = logits.argmax(2)[-1].item()\n",
        "\n",
        "    print((' '.join([tokenizer_ru.id_to_token(i).replace('##', '') for i in outputs[k:]])))\n"
      ],
      "id": "4a21a9c8",
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "db3a19c3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b5629dd-8c16-45e3-93ce-f1dbc4a2a0b7"
      },
      "source": [
        "translate_works(\"Example sentence is here. My work is done. I am tired.\")"
      ],
      "id": "db3a19c3",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Пример , здесь . Пример моего труда . Пример ю .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f284d0f1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87fa49b3-389c-4339-cb9f-4c67c7cf8290"
      },
      "source": [
        "translate('Can you translate that?')"
      ],
      "id": "f284d0f1",
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Ты можешь это пере водить ?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFhBNBDvLTgH"
      },
      "source": [
        "def translate(full_text):\n",
        "\n",
        "    sent_text = nltk.tokenize.sent_tokenize(full_text)\n",
        "    outputs = [] \n",
        "    inputs = []  # тут один тензор\n",
        "\n",
        "    k = 0\n",
        "    for text in sent_text:\n",
        "      input_ids = [tokenizer_en.token_to_id('[CLS]')] + tokenizer_en.encode(text).ids[:max_len_en] + [tokenizer_en.token_to_id('[SEP]')]\n",
        "      output_ids = [tokenizer_ru.token_to_id('[CLS]')]\n",
        "      inputs.append(input_ids)\n",
        "      outputs.append(output_ids)\n",
        "      k += 1\n",
        "\n",
        "    input_ids_pad = torch.nn.utils.rnn.pad_sequence([torch.LongTensor(i) for i in inputs]).to(DEVICE)\n",
        "    output_ids_pad = torch.nn.utils.rnn.pad_sequence([torch.LongTensor(i) for i in outputs]).to(DEVICE)\n",
        "\n",
        "    (texts_en_mask, texts_ru_mask, \n",
        "    texts_en_padding_mask, texts_ru_padding_mask) = create_mask(input_ids_pad, output_ids_pad)\n",
        "    logits = transformer(input_ids_pad, output_ids_pad, texts_en_mask, texts_ru_mask,\n",
        "                         texts_en_padding_mask, texts_ru_padding_mask, texts_en_padding_mask)\n",
        "    \n",
        "    for logit in torch.flatten(logits.argmax(2)):\n",
        "      pred = logit.item()\n",
        "\n",
        "      while pred not in [tokenizer_ru.token_to_id('[SEP]'), tokenizer_ru.token_to_id('[PAD]')]:\n",
        "        outputs.append(pred)\n",
        "        output_ids_pad = torch.nn.utils.rnn.pad_sequence([torch.LongTensor(i) for i in outputs]).to(DEVICE)\n",
        "\n",
        "        (texts_en_mask, texts_ru_mask, \n",
        "        texts_en_padding_mask, texts_ru_padding_mask) = create_mask(input_ids_pad, output_ids_pad)\n",
        "        logits = transformer(input_ids_pad, output_ids_pad, texts_en_mask, texts_ru_mask,\n",
        "                             texts_en_padding_mask, texts_ru_padding_mask, texts_en_padding_mask)\n",
        "        pred = logits.argmax(2)[-1].item()\n",
        "\n",
        "    print((' '.join([tokenizer_ru.id_to_token(i).replace('##', '') for i in outputs[k:]])))\n"
      ],
      "id": "jFhBNBDvLTgH",
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_WoUziMBLpb0",
        "outputId": "01ee6582-2a53-429d-ad3a-aacede536840"
      },
      "source": [
        "translate(\"Example sentence is here. My work is done. I am tired.\")"
      ],
      "id": "_WoUziMBLpb0",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-86521a0ef654>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Example sentence is here. My work is done. I am tired.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-29-93ee4c106e56>\u001b[0m in \u001b[0;36mtranslate\u001b[0;34m(full_text)\u001b[0m\n\u001b[1;32m     31\u001b[0m         texts_en_padding_mask, texts_ru_padding_mask) = create_mask(input_ids_pad, output_ids_pad)\n\u001b[1;32m     32\u001b[0m         logits = transformer(input_ids_pad, output_ids_pad, texts_en_mask, texts_ru_mask,\n\u001b[0;32m---> 33\u001b[0;31m                              texts_en_padding_mask, texts_ru_padding_mask, texts_en_padding_mask)\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-216c3930c48d>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, trg, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0msrc_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpositional_encoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc_tok_emb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;31m#         print('pos inp')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0mtgt_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpositional_encoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtgt_tok_emb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;31m#         print('pos dec')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None,\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-216c3930c48d>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0memb_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;31m# Seq2Seq Network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    156\u001b[0m         return F.embedding(\n\u001b[1;32m    157\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   1914\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1915\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1916\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1917\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: index out of range in self"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQXyli-oO3yR"
      },
      "source": [
        "Вот полностью переведенный текст новости. Сразу видно, что слова по символам часто бьются или по морфемам"
      ],
      "id": "wQXyli-oO3yR"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b234717b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42d2a45e-21ba-4536-90ed-fd0747c69592"
      },
      "source": [
        "translate(en_text)"
      ],
      "id": "b234717b",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Российская По ет Дмитрий Бай ков Целевой группой по На ва льным вопросам 9 июня 20 21 Ф Б России в декабре 2020 года , проведение расследований « Пер сона льных « Пер сона ».\n",
            "Три члена этой группы – включа ющие хими ческие эксперты , медицинские врачи и опера тив ники безопасности – были т ене ва льные , Ново си бир ски и на пала те и на\n",
            "На ва льный упа док упал на пол на авиа компании Тома ски на 21 августа 2020 года , из которых три европейских лаборато рий и были убиты .\n",
            "Телефон одного члена группы Ф Б был гео расположен в нескольких минутах ходьбы от На ва льного расстояния до того , как он упал .\n",
            "Последующие расследования , выя вленные кот , выя вленные значительные отношения между членами группы по этой должности Ф С , и ранее не объяс нили я дови тые или смер тность от нес енных от\n",
            "Другие вероят ные цели включали два активи стов по правам человека на Кавка зе , а также анти корруп ционный активи ст .\n",
            "Поскольку число расследований выросла , такая схе ма , возник шая в каждом случае , когда в каждом случае , когда они были приняты меры по борьбе с ними : секре тным опера тором тай\n",
            "На первых этапах отра вление цель отра вления , будет иметь хвост членов второй службы , а эксперты по уголовным делам , как правило , присоеди няется к ним .\n",
            "В каждом случае члены группы по обе Ф Б В участвовали в оконча тельном , оперативных поездок , в ходе которых жертва была отра жена .\n",
            "Кроме того , в каждом из случаев , выя вленных Б инг кот , отра вление было проведено вне Москвы , обычно в ходе поездки жертвы , чтобы она могла быть отра влен ным в\n",
            "Не ясно , является ли эта схе ма предпоч тения предпоч тения в отношении отра вления целевых показателей в уда ленных местах – где доступ к личной жизни жертвы .\n",
            "Об следование показа ло Дмитрий Бай ков ( c ): « Рас следование кот Бу ква и его партнеры по проведению расследований и след ственных партнеров опреде лили кассет ные поездки и поезда ми ,\n",
            "Поезд ки от ска зов состоялся в период с мая 2018 года и апрель 201 9 года и куль мина ции в Ново си бир ске 12 апреля 201 9 года .\n",
            "Две члены отдела Ф С Б – один из второй службы , а один из Уголовного кодекса , путеше ствующих по различным вопросам , которые были представлены в Ново си бир ске , путеше ствовать\n",
            "Дмитрий Бай ков , затем выле тел в Екате рин бург в следующий день и на У фа следующее утро .\n",
            "Он упал в пол ё та в самолё те в У фа , начал р во та не контролиру емой и в конечном итоге потерял сознание только после того , как приземли лся самолет .\n",
            "Он остался в коме в течение пяти дней , страда ющих от того , что первоначально диагности ровали в качестве моз говой системы и крити ческих уровней крови .\n",
            "Он был разме щен на искус ственной венти ляции и ле чен в случаях симпто мов , а также в рамках анти био тики против неизвест ного источника \" бак тери аль ного отра вления\n",
            "После сохраня ющихся вмеша тельств со своих коллег Ново aya Газа eta , и несмотря на первоначальный на препятствия власти , в конечном итоге пере вез ло в Моск ву институт , где он был\n",
            "На следующей неделе улучши лось положение , и он был освобо жден от 26 апреля , несмотря на то , что его внезап ное невро логи ческое не известный .\n",
            "Первонача льно врачи считали , что он может пострада ла ин суль т , но потом измени ла диагно з \" не указанного повреждения мозга \", связанные с диа бет ским ти змом , и\n",
            "В больнице , документ о том , что а трибу ты медицинского назначения для не известных бак тери аль ных продуктов .\n",
            "К ков ков говорит , что его экстрен ный доктор , который сообщил ему , что они не могут найти источник отра вления .\n",
            "Эксперты по хими ческому оружию , консульти ровались с экспертами Б ё ко , подтвер див , что у ко ков жест кие медицинских симпто мов могут быть разум ными , невро логи ческими последствиями\n",
            "Дело Дмитри я Бай ков , предполага емое отра вление , свидетельствует о том , что Алек се ем На ва льный , включая расширен ный FS B хвост за период с 1 июля 1999\n",
            "Кто Дмитрий Бай ков Дмитрий Бай ков – известный российский писа тель , по эт и журнали стом .\n",
            "Р ено й , как литера тур ный крити к , преподава тель и си р , он часто хи ти ри рованные и си ними сти хи – в том числе посредством само само\n",
            "К ков ков - крити ка российского правительства и дважды отказа ли личные приглашения с президентом Пу тина в качестве части президента Президента Российской Федерации , как и в качестве представителя президента , так и\n",
            "Он активно оппози ционный активи ст , принима ющий участие в и ко - орган , организо ванный анти правительственные проте сты .\n",
            "В ходе начального выборов в связи с оппози ционным советом в 2012 году ков получил второе число голосов после проведения Алек се ем На ва льного .\n",
            "В 2013 году К сожалению , в результате этой операции , которая стреми лась к полити чески не кредито ванию П го ж ского бизнеса , бизнес мен ами лия и не только в 2013\n",
            "По словам бывшего П го го ж ского университета Андрей Миха ила ва , П го ж жа в рассмотрел « второй после На ва льного [ среди оппози ционных деятелей ] в составе представителей\n",
            "К ков ков ранее выступа ла против приложения России и его вмешательства в восто чное Украине и проте сто вал в тюрьме украин ского кино фильма О лег .\n",
            "Несмотря на у ще слав ные анти К рем ля и анти создание вид ений , ни своих коллег не о том , что Б ит ва ко , ни сам не сам , ни\n",
            "В разгово ре с Б лай лом и Ин с дер , Дмитрий Бай ко п вел л , что \" моти вы К рем ля не обязательно знают , что \" моти вы К\n",
            "Группа из группы Ф Б Б первый документа льный фильм о доказательства х B ков , который был у реза н от ар мей ского полицейского контроля от пере стре ла ков из группы по\n",
            "В 9 : 35 pm в тот день , К ков ская часть России была поле зна в стан .\n",
            "Он посетил город , чтобы доставить лек цию детей и родителей на важное значение книги детей , которые были постро ены в качестве книги Кар лс сона о том , что они не могут быть\n",
            "Несмотря на не под вер ность целей этой поездки два члена анти экстреми зма , отдел контроля Ф Б Т является причиной того , что они не являются результатом этого визита и не являются экстреми\n",
            "Это были Владимир Па ния ев , путеше ствующих по его настоящему имени , и Вале рий Су ха ва , который путеше ствовал под прикрытием Нико лай Го ри .\n",
            "Про тиво ре чный образ для события с Дмитрий Бай ков .\n",
            "Дмитрий Бай ков , заброни ровал свой самолет в У фа 12 мая 2018 года .\n",
            "Па ни е ев и Су ха вы сня ты билеты 18 мая 2018 года , и не первоначально книгу возвра та .\n",
            "По запися м , которые они оба заказа ли билет в Ю фа , после того как в Ю фе , сразу после того , как К ву ков закончи лся лек цию о том\n",
            "Они заказа ли пол день в следующий день , пока К ву ков при летел в 11 часов , и вечером .\n",
            "За брони руйте данные для Влади мира Па я ев и « Нико лай Го хов » в мае 2018 года .\n",
            "Цель этого первого пути к восто ку не яс ности .\n",
            "Как и в других случаях отра вления ди сков Б рас следу ется , эта первоначальный поездки вовле чен только в анти экстреми зм Ф Б , не только в рамках анти экстреми зма ,\n",
            "Именно так и было , что жена К ек то вич Э ки ева стала говорить , что она начала наблюдать за первым знаком ением .\n",
            "Она помнит , что в мае 2018 году , она неоднократно видела человека в доме напротив , в направлении их квартиры с би нок лем .\n",
            "Она сказала , что пома хала ему в несколько раз , когда заметил его , и каждый раз он пры гнул от глаз .\n",
            "После разрыва примерно 6 месяцев , в ходе которых два офицера Ф Б не посетил Моск ву , 15 ноября 2018 года они были сня ты на билеты на две должности сотрудника по вопросам безопасности\n",
            "Па ни е ев был первым , чтобы сделать заказ в 2 : 34 часов ; час спустя , в 3 : 39 pm его стар шему колле гу Су ху Су ка Су ка\n",
            "Однако , похоже , он купил билет на не тот рейс – и два минуты спустя он отмени л билет и купил новый , за исключением тех , кто получил билет .\n",
            "Вы лет бронирования для Влади мира Па я ев и « Нико лай Го хов » в ноябре 2018 года .\n",
            "14 ноября 2018 года – всего лишь день до того , как эта Ф С купила билеты , Дмитрий Бай ков заказа л пол на один самолет и на один раз .\n",
            "Однако он лети т из Санкт - Петербурга до Ро стов , как он плани ровал визит в этот город 16 ноября .\n",
            "Он приземли лся в Ро стов всего через час и полу до полу до команды FS B тень .\n",
            "При бы тие рей са ций в Ро стов на Дон 18 Ноября 2017 .\n",
            "Как и ранее поездка в У фа ву , Ро стов - Ро стов был назначен вечер литера тур ных и му сков на русском классе .\n",
            "Два офицера Б Б , заказа нные на самолет в Моск ву в 4 : 34 вечера и при летел в Моск ву в следующий день в 12 : 55 pm .\n",
            "К рова во уле в Моск ву 9 часов спустя , в 10 : 00 часов 18 ноября .\n",
            "Еще шесть месяцев прошло без того , как очевидно , известно , что Ф АС Б .\n",
            "Потом автор планирует новый лек цию до Ро стов на 7 апреля 201 9 года .\n",
            "В этот раз , К ков ков поста вит два переговоры – один из э ст ети ст - вселенной Гарри Пот тер ской , а еще один о истории совет ского и э ст\n",
            "Он заказа л самолет из Москвы до Ро стов в 5 : 45 pm 6 апреля , а возвра щаясь в 7 : 45 pm на 8 апреля 201 9 .\n",
            "В ночь 4 - 5 апреля 201 9 года Екате рина К ев ши швили говорит , что она получила одно временное опове щение от всех своих посы лок , что кто - то из\n",
            "Она помнит , что её теле грам мы успешно про мети ровали .\n",
            "Позже , 5 апреля , один и один из знако мых офицеров FS B второй службы , заброни рованных в Ро стов на следующий день .\n",
            "Они приземли лись в Ро стов ском городе в 5 : 30 часов , прежде чем К ков при будет .\n",
            "Па ни ев и Су ха рев ( снова с использованием его изменение эго « Гор хо ва ») не заказа ла билет на возвра т .\n",
            "Однако , брони руйте данные , что поздно ночью – в 1 - м от 7 апреля – они купили билеты в Моск ву за 11 : 50 pm\n",
            "Таким образом , два офицера Б Б - Ро стов , не оста вив на работу по выполнению работ B y ков позже , после полу дня .\n",
            "В Сочи между Сочи и Сиби ри три дня спустя три дня , она и нер вы до этого до не дели тся от туристов .\n",
            "Утром 11 апреля 201 9 Су ха снова путеше ствует под прикрытием , лет назад в Сочи , в Сочи , на один способ , и остался там жить .\n",
            "Эта ночь , когда ков ская по говорили о Э хо го Москвы .\n",
            "Он говорил о своей “ политических шу ток ” в Ро стов ском Ро стов и о планах расширения на этой темы в предстоя щем ле ле ле ле ле .\n",
            "Он говорил о том , почему Владимир Путин – к которому « общество по ощу щению уда ляется от нашего мира , как иностран ца », редко появляется в качестве цели политического политического кризиса .\n",
            "Он также представил свою регуляр ную крити ку отсутствия верховенства права в современных России .\n",
            "В следующий день 12 апреля два офицера Ф Б – Су ха ва в Су х ва Владимир Па ни ев из анти экстреми зма .\n",
            "К этому времени они знали , что К ву ков планирует летать в Ново си бир ск позже той ночью на первое леги оне ку си бир ское Си бир ское Си бир ское море\n",
            "Не менее , как из этих опера ционных опера ционных опера ционных опера ционных систем , так и « Иван Спа й ро дон », так и « Иван Спа й ро дон ».\n",
            "Это была первая поездка Па ни ев под прикрытием , когда он был у него в ско ром времени Дмитрий Дмитри я Бай ков ( последний раз использовал эту личность в 2017 году ).\n",
            "Аэро ф лот S U 15 48 лет прибыл в Си бир ский город в 4 : 40 утра .\n",
            "К рова во пришел три и полчаса позднее , в 9 : 05 pm .\n",
            "Время прибытия в Ново си бир ск 13 апреля 201 9 года .\n",
            "Ново си бир ск , Ново си бир ск , Дмитрий Бай ков и Екате рина К ев ченко превра тил свой путь в город и провери ла в центр города дома Дом на Отель\n",
            "В течение всего 1 : 00 часов они оставили номер отеля и ходили в кино театр Побе ды , в 10 минутах ходьбы от отеля .\n",
            "В тот день , в зале , на котором планируется участие почет ного чита теля в T ot al D ict ant ev ent , занима ющегося в коридо ре – ежегод ным конкурсе по\n",
            "Дмитрий Бай ков на сцене в Ново си бир ске .\n",
            "Фото графия , которая была на кана ле .\n",
            "В настоящее время в общем D ict ant D ict ant ev ent занимает один час чистого чтения , и обычно примерно через два часа .\n",
            "В интернете было про писа но онлайн , позволя ющее пользователям уда ленно следовать чт ению .\n",
            "Таким образом , было ясно , что К ву ков – и его жена , сопровожда вшая его всех событий – не возвра тится в отель на протяжении всего периода времени .\n",
            "На самом деле , пара не возвра тилась в номер в гостинице до 6 : 00 .\n",
            "Они также оставили их комнату еще один раз в 10 : 00 вечера , и вернулась обратно в полно чь .\n",
            "Дмитрий Бай ков в мероприятии в Ново си бир ске .\n",
            "Фото графия , которая была на кана ле .\n",
            "После полу ночи в местное время , Пан сии в городе А си пов одновременно провели время в Москве авиа компании F O s Pan ia ev и O s i пов одновременно с оговор\n",
            "Про сы тные сведения о путеше ствиях в Владимир Па ни ев и Иван О си пов 13 апреля 201 9 года .\n",
            "Предыду щая операция ?\n",
            "Отель Дом ina в Ново си бир ске является частью итальян ской сети отеля .\n",
            "13 августа 2020 года директор расследований деятельности по борьбе с коррупцией Алек се ем На ва льного фонда , Мария Пи ев ч ха , прибыл в Ново си бир ск и г .\n",
            "Пи ш ха был путеше ствен ником в один день на На ва льном и заметил , что она была съем ка по пути из аэропорта .\n",
            "Несмотря на несколько ма нев р с пытается сбежать от своих последова телей , она нашла их в отеле , когда она при была .\n",
            "В собственных слов Пи ви ш х шей х шей х , в ходе проверки , она зада ла необы чное и бес преде льные вопросы , включая , что ее оккупа ция была .\n",
            "После того как она провери ла , она нашла её комнату для отеля , подели лась с другой комна той , и просила пере мести ться в другой номер из проблем безопасности .\n",
            "Отель снизи лась на ее просьбу , несмотря на – после проверки доступности бронирования . com – найти тот , что отель был наполовину пу ст .\n",
            "После этого П ев шей ха , не про ехали из отеля без проверки и без информи рования персонала отеля .\n",
            "Позже той ночью телефон одного из членов FS B яд , Алексей Александ ро вич , пин , под ходя сь перед До мой ной гостиницы .\n",
            "Не ясно , если это результат Александра останется в том , что отель или попытка отследить Мария Пи ев ха .\n",
            "К услугам гостей с вопросами о доступности и государственной безопасности в период времени К БР Р .\n",
            "К пресс - служба отеля не ответила на вопросы .\n",
            "В следующий день Дмитрий и Екате рина провел в городе с До мо ной . 14 апреля , Дмитрий и Екате рина провели в основном из отеля « Дом ina ».\n",
            "Утром 15 апреля они ле тели в Екате рин бурге , и после лек ций в У раль ском городе , два дня , два дня , и два дня , в г .\n",
            "Доро жные структуры Дмитрий Бай ков и членов группы FS B .\n",
            "16 апреля Дмитрий и Екате рина прос на около 8 : 30 утра .\n",
            "Они завтра ка вали с Ев гений Рой з ман , еще один оппози ционный активи ст и только после 10 : 00 им пришлось взять такси до аэропорта .\n",
            "Они прибыли 10 : 25 утра , только час до вылета .\n",
            "Дмитрий не пил или есть что - нибудь в аэропорту , а после форма льности , возглавля ющих от ходы , направля емые в область .\n",
            "На пути к ворота , только до 11 : 00 утра , и два - и - на - пол часа после того , как он проснулся и вста вил на улицу .\n",
            "Несмотря на эти ран ние симптомы , К сожалению , проте ст на борту самолета .\n",
            "В период само лёт был более тяже лым и его кожа был покры т большим количеством ка пель си нов .\n",
            "В своих коллек циях , подтвержда емых Екате рина , его ум « пере ключи лся и от » – периодически закры вает глаза и ста п ения .\n",
            "После некоторых лет он сло ман на ал та рю , объяс няя , что это сделал его лучше .\n",
            "Не , конечно , это зер кала реакция , которую Алексей На ва льный сказал , что у него был после его под ка мени с Ново си бир ского аген ства с лета тельного\n",
            "Как будто На ва льный , К ков ский , говорит , что он ничего не помнит от момента , когда он лежа л на само лете , и как он , он сделал это\n",
            "В отличие от На ва льного , который в данном смысле стал дель ным и последова тельным , ум ён ным , не менее чем до конца самолета до конца лета .\n",
            "Как сообщили в У ТА ir самолё те , перевоз я щих все больше , бл ён ные бу льдо г ков был при зем лен в У фа ре , записи о путеше ствиях\n",
            "После посад ки , Бай ков не мог стоять на ноги и мед ки тя нули его в скорую .\n",
            "На пути в больницу У фа , он чувствовал себя очень горя щим , и взял его руба шку .\n",
            "Это была в ско рая , что потерял потенциал слова .\n",
            "В описа нии его жены , что - то , что он за лез ет в сви дан ность , в последова тельном зву ке .\n",
            "В то же время он сохра нил свою способность понять речь других .\n",
            "В собственной коллекции , субъек тивного опыт , который он имел после того , что он считает его отра жением , по аналоги и тем самым , он считает , что он был отра влен\n",
            "субъек тивный опыт не только сход ство между двумя случая ми .\n",
            "Б ит ва за К ву ков в слова Дмитри я Мёр тов , затем реда ктор - главный оппози ционный банк « Ново aya » Га та , как только в ско б ках\n",
            "План был должен был транспорт Бай ков в специаль ном невро логи ческом кли нике в Москве , Институт Бур ден ко .\n",
            "Однако M ur at ov сказал нам , что меньше часа до самолета Y ak ov le v ing E fa , достиг пило та , через воздух , направля емые авиа перевоз чиками ,\n",
            "Пи лот информи ровал Му ра тов , что если он не получил инструкций по нару шению , в течение 10 минут он должен выполнить и пере вернуть самолет .\n",
            "В этом пункте Му ра тов и его коллег иниции ровали стру ю связь , в том числе к числу К рем ля .\n",
            "M ur at ov говорит , пока не он не уверен , что позво нит в результате , но в конце концов кто - то про инстру кторов по поводу У фа и пере мести\n",
            "Ра дар 24 следы меди та , лета ющих в У фа .\n",
            "В то время , как указано в случае , приведен ных в россий ском министер стве здравоохранения , не желает разрешения на транспорт ков в Моск ву в силу своего « серьезное состояние ».\n",
            "В конечном итоге , после общественного давления со коллег Бай дена Бур дена удалось получить четкое официальное утверждение в больнице Бай дена .\n",
            "Не правильно диагности ческая рез кто , которых говорят о том , что Ин с дер показал эпи ческое эпи ческое рис ( с его разрешения ) подтвердил , что « Ин си дер с\n",
            "Эти ми гипер спрята ны за ( проф use per sp ect ive ) и ни мо кар дия с ги по ле мия , т . е .\n",
            "a тяже лой снижения объема цирку ляции крови на фоне сохрани вшихся рит м сердце ( пуль с ков был быстрее , чем на На ва льного фильма « На ва льный » ( пуль\n",
            "В рачи , которые просили быть аноним ным но , также сказали , что B y ov ly ce mic profile показа ло ви тации , показали , что они отобра жаются на На ва\n",
            "Однако в то время врачи Э М Э М , объяс няли эти изменения на побо чные последствия бар би та , внед ря емые в систему пациент ки в ходе операций пациента .\n",
            "В рачи , которые ле чили К уби не , не опреде лили причину отра вления , и обраща лись к нему исключительно симпто мати ческому мати ческому .\n",
            "Поскольку предо сторо жности против возможной ( но не выя влены ) бак тери аль ных инфекции , он был дан анти био тики .\n",
            "После нескольких дней в коме , К ков ская кон тен тация сознания , но остался в состоянии дис пет чер ской области – аналоги чно , наблюда вшаяся в На ва льном направлении .\n",
            "В апреле 201 9 года врачи ле чили к ков у Институт но ко нен ко не может согласиться с чув ством консенсуса .\n",
            "Те ги гипоте за гостеприим ства для гостеприим ства не могут быть проверен ы , как уровень шума ков ков че хо ль не был мери лом после того , как уровень шума в измер\n",
            "В отсутствие раннего проведения расследований и более глубокого понимания контек ста , однако врачи , которые в настоящее время решили написать инцидент как не известный пи си му ф .\n",
            "В то же время врачи согласны , эндо ген ная причина для ке до кисло та , привело бы гораздо более дли тельное воздействие на тело – это может привести к пере ходу .\n",
            "Кто же Иван О си пов , Вале рий Су ха ва и Владимир Па ни ев ?\n",
            "Все три офицера Б Б , которые х вали Дмитрий Бай ков также Т ень е Алексей На ва льный до его 2020 года отра вления и два из них – Пана фрикан ского фонда\n",
            "Иван О си пов - это укра шен офицер Б Б , который , судя по его сети со стороны туристов и телефон ного сообщения , интегри руется с Ф С Б .\n",
            "Основы ваясь на анализе телефон ных звонков с двух месяцев вокруг На ва льного отра вления в 2020 году , более 70 % звонков были до других офицеров с использованием информации , а также на\n",
            "Он также до води л до сведения ученых из Научно - исследовательского института , которые связаны с разработкой нер вов .\n",
            "Данные из открытых источников , включая ныне - де жу то вый план О d no kl ass n kl ass n , предполага ющие О си пов ский окон чил Москов ский состав Москва\n",
            "Его зовут в 2008 году список жителей Москвы , имеющих право на получение пособий ( обычно из - за вете ран ского военного статуса или военного решения ).\n",
            "В период с 2008 по 2015 годы его основной поездки было площади в Че чне , где в Че чне были тысячи поездок в Че чне .\n",
            "Во многих поездок в этот регион О си пов сопровожда лся членами Группы Н Д В 36 39 1 или контр террористической деятельности , а также с учетом их последствий .\n",
            "В своих последних поездок он обычно сопровожда лся членами Уголовного кодекса , включая Александра , Т эй на и Ки ву в Ке д ри ву с .\n",
            "Владимир Па ни ев является членом анти экстреми зма ского подразделения второй службы .\n",
            "Мет ал ло данных из одного из его телефон ных номеров показывает , что его главная работа находится в 12 V ern ad sk o go B le – штаб - квартиру из группы .\n",
            "Пана фрикан О си пов и Алексей Александ ро вич из Уголовного кодекса Ф Б , хвост , восто чное строительство и строительство , строительство и строительство , строительство и строительство , строительство и строительство\n",
            "В течение этого периода Пана фрикан ское агентство по вопросам безопасности и развития , в частности , « Ген . Бо гда на » ( M ak en an ov , M ak en o\n",
            "Вале рий Су х рев — офицер Ф Б родился в 1956 году .\n",
            "Основы ваясь на анализе его поездок и связи , похоже , он стал стар шим офице ром Ф Б Б , участву ющим в серии отра вления оппозиции , и в связи с этим вопросом\n",
            "Су х хвост Влади миру Кара - Мёр за в 2015 и т ене вал его в Каза ни два дня до его первого отра вления в ней .\n",
            "После того как предполага лось тай ство Нико лай Го лов , родился 19 55 года , он хвост Алек се ем На ва льного на самом деле , в основном его президент ский кандидат\n",
            "Он использовал тот же тай ный иденти чность , который должен быть х восто м Д . К . к 2013 году и 201 9 .\n",
            "В течение последних дней , ведущих к На ва льного отра вления в 2020 году Су ха в десят ках звонков на многие сотрудники Ф Б , связанные с этим , связаны с угрозой исчезновения\n",
            "Однако он сделал еще больше телефон ных звонков – и путеше ствует по более совмест ному рей шему рей шему авиа перевоз кам с команди рами второго класса С Б , который предлагает всем получить\n",
            "Как О си пов , Су х ва был включен в перечень , озаглавлен ную « О си пов в Москве в Москве в базе данных , проведен ную правительством Ката стро .\n",
            "Пере говоры кот и Ин си дер попыта лись связаться с тремя опера тивными опера тивными средствами через телефон ные номера , использу емыми ими в ходе На ва льного отра вления в 2020 году\n",
            "Ни один из трех номеров не ответил .\n",
            "Пере говоры кот также присла ли вопросы к ним через теле грам му , но не получил ответа на время пресс - службы .\n",
            "Новые In s s of FS B ’ s Po ison ing Mod us T er and y T he ar ing into Дмитрий B y k ov ing re pl ac ed in de\n",
            "Как и в предыдущих миссиях группы , группы по отра влению в составе двух различных департаментов Ф Б , анти экстреми зма , подразделение второй службы по борьбе с экстреми змом .\n",
            "В этих межсектораль ных групп второй Служба представляется веду щей роли , при которой его члены сле дят за целью за период времени , когда они будут иметь возможность проводить обзор этой деятельности .\n",
            "Ва жным роль Вале рий Су ха в операциях отра вления также стало яс ным после того , как скоро будет обнару жено его тай ство .\n",
            "Суще ствование « Гор хо хо вым » как член команды первоначально был обнару жен после того , как он был найден в том же время как Па ни я и Па ния .\n",
            "Первонача льно , что Б лай т подтвердил , что нет реального человека с таким точ ным именем и рождения .\n",
            "По сравнению с лета ющими рей сом « Гор хо шо ва » и другими извест ными членами Ф Б показали идеально мат с реальной иденти чности Су ху .\n",
            "« Гор хо ва » только поле тели , к которым Су ха ва также совершили про лет , и не было временной части чной части / конфликт ).\n",
            "Последнее , после того как мы получили звонки в архи ви ю Су ха в течение одного конкретного месяца , мы по сравнению с звонки в путеше ствиях в историю путеше ствий .\n",
            "После установления личности « Гор хо ва » как Су ха ва , мы соответ ствовали двум совокуп ным поездок в связи с теми , кто Алек се ем На ва лом Ка сса н\n",
            "Его важную роль в работе 2020 года ( во время которой он не стал путеше ствовать )\n",
            "Другой вид оперативного плана , который требует дальнейшего расследования , является роль Сочи в качестве важного паралле льного паралле льного назначения во многих операциях отра вления .\n",
            "Данные о путеше ствиях показывают , что в ходе заключи тельного этапа нескольких поездок по отра вления , отдельной команде от отдела яд , которые не разверну ты на пути к отсле живанию .\n",
            "Роль этого положения , независимо от того , является ли в качестве материально - технического центра или координационного центра , пока не яс ны .\n",
            "Рас следование в Бай ков , предполага емое отра вление , привело также к дальнейшему повышению уровня выбросов и метода подачи заявления .\n",
            "Если вещество , которое выз вано отра жением ков , является гостеприим ным аген том , который был отра жен в гостеприим ной форме , то это вполне прав до подобно тому , что оно\n",
            "По словам британ ского и бол гар ского законодательства , метод применения в Ско рей аль и Ге бре ви ва ( предположительно используется сотрудниками GR U ) ( по видимому , использу емым сотрудниками\n",
            "В случае с использованием На ва льного отра вления , связан с одной командой FS B , метод применения , скорее всего через мо рон или пома за ния агента по отношению к этому делу\n",
            "Если один же метод применения ( и общий класс нер вов нер вов ) используется одной командой в Бай ков ской операции более чем в начале года , то более чем в два раза больше\n",
            "В этом сцена рии он впервые был подверг нут нер ви ному агент ству на утро 16 апреля 201 9 года , когда поса дили на зара женной части ниж него белье .\n",
            "Две эксперты по хими ческому оружию провели консультации с другими экспертами по вопросам оружия , подтвердили , что содержание класса Но вых классов будет оставаться актив ными с минима льными не постоян ством и ухуд\n",
            "Если этот подход был действительно использоваться в сотрудничестве с ков ской операции , то время между ним при контак том с загряз нен ным количеством одежды и первым симпто мов .\n",
            "Это расследование было проведено в партнерстве с Ин си дер России и Дер Спа й ге ля .\n",
            "Крис Г роу за и Й ор дан та Т са лов служи ла главным исследова телям для проведения ин га тора .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d402783c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5382b7db-c126-4ff3-f854-19c0f512e0ca"
      },
      "source": [
        "print(translate(en_text[:491]), en_text[:491])"
      ],
      "id": "d402783c",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Российская По ет Дмитрий Бай ков Целевой группой по На ва льным вопросам 9 июня 20 21 Ф Б России в декабре 2020 года , проведение расследований « Пер сона льных « Пер сона ».\n",
            "Три члена этой группы – включа ющие хими ческие эксперты , медицинские врачи и опера тив ники безопасности – были т ене ва льные , Ново си бир ски и на пала те и на\n",
            "None Russian Poet Dmitry Bykov Targeted by Navalny Poisoners\n",
            "June 9, 2021\n",
            "FSB\n",
            "Russia\n",
            "In a December 2020 investigation, Bellingcat and its partners identified seven FSB officers who had tailed Russian opposition leader Alexey Navalny on more than 35 trips around Russia since early 2017. Three members of this group – which included chemical weapons experts, medical doctors and security operatives – had shadowed Navalny to Novosibirsk and onward to Tomsk, during his August 2020 trip to Siberia.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOcguRLYQCxd"
      },
      "source": [
        "Ну недообучился трансформер в общем, если на подольше его оставить учиться, думаю, было бы лучше"
      ],
      "id": "QOcguRLYQCxd"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9U3nEDy9PVgV"
      },
      "source": [
        "# old old old\n",
        "def translate(full_text):\n",
        "\n",
        "    sent_text = nltk.tokenize.sent_tokenize(full_text)\n",
        "    outputs = [] \n",
        "    inputs = []  # тут один тензор\n",
        "\n",
        "    for text in sent_text:\n",
        "      input_ids = [tokenizer_en.token_to_id('[CLS]')] + tokenizer_en.encode(text).ids[:max_len_en] + [tokenizer_en.token_to_id('[SEP]')]\n",
        "      output_ids = [tokenizer_ru.token_to_id('[CLS]')]\n",
        "      inputs.extend(input_ids)\n",
        "      outputs.append(output_ids)\n",
        "\n",
        "    input_ids_pad = torch.nn.utils.rnn.pad_sequence([torch.LongTensor(input_ids)]).to(DEVICE)\n",
        "    output_ids_pad = torch.nn.utils.rnn.pad_sequence([torch.LongTensor(output_ids)]).to(DEVICE)\n",
        "\n",
        "    (texts_en_mask, texts_ru_mask, \n",
        "    texts_en_padding_mask, texts_ru_padding_mask) = create_mask(input_ids_pad, output_ids_pad)\n",
        "    logits = transformer(input_ids_pad, output_ids_pad, texts_en_mask, texts_ru_mask,\n",
        "                         texts_en_padding_mask, texts_ru_padding_mask, texts_en_padding_mask)\n",
        "    pred = logits.argmax(2).item()\n",
        "\n",
        "    while pred not in [tokenizer_ru.token_to_id('[SEP]'), tokenizer_ru.token_to_id('[PAD]')]:\n",
        "        output_ids.append(pred)\n",
        "        output_ids_pad = torch.nn.utils.rnn.pad_sequence([torch.LongTensor(output_ids)]).to(DEVICE)\n",
        "\n",
        "        (texts_en_mask, texts_ru_mask, \n",
        "        texts_en_padding_mask, texts_ru_padding_mask) = create_mask(input_ids_pad, output_ids_pad)\n",
        "        logits = transformer(input_ids_pad, output_ids_pad, texts_en_mask, texts_ru_mask,\n",
        "                             texts_en_padding_mask, texts_ru_padding_mask, texts_en_padding_mask)\n",
        "        pred = logits.argmax(2)[-1].item()\n",
        "\n",
        "    print((' '.join([tokenizer_ru.id_to_token(i).replace('##', '') for i in output_ids[1:]])))\n"
      ],
      "id": "9U3nEDy9PVgV",
      "execution_count": null,
      "outputs": []
    }
  ]
}